epochs: 100
train_batch_size: 512
learner: adam
learning_rate: 0.001
train_neg_sample_args:
  {
    "distribution": "uniform",
    "sample_num": 1,
    "alpha": 1.0,
    "dynamic": False,
    "candidate_num": 0,
  }
eval_step: 1
stopping_step: 10
clip_grad_norm: None
weight_decay: 0.0
loss_decimal_place: 4
